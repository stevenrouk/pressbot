{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "vegan subreddits:\n",
    "- https://www.reddit.com/r/vegan/\n",
    "- https://www.reddit.com/r/animalrights/\n",
    "- https://www.reddit.com/r/animalwelfare/\n",
    "- https://www.reddit.com/r/veg/\n",
    "- https://www.reddit.com/r/vegetarian/\n",
    "- https://www.reddit.com/r/vegetarianism/\n",
    "- https://www.reddit.com/r/dietaryvegan/\n",
    "- https://www.reddit.com/r/veganrecipes/\n",
    "- https://www.reddit.com/r/vegproblems/\n",
    "\n",
    "location-based vegan subreddits:\n",
    "- https://www.reddit.com/r/VeganDenver/\n",
    "- https://www.reddit.com/r/vegan/wiki/localvegansubreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. Get some Reddit data into a database\n",
    "    - python scrape_reddit.py\n",
    "    - eventually: have a script continuously running, scraping subreddit data as it comes in and updating with new comments and such\n",
    "2. Create a pipeline for:\n",
    "    - reading in the data\n",
    "    - training a model on the data\n",
    "    - saving the model out to disk\n",
    "    - transforming the input data to topics and topic space\n",
    "    - saving the transformed data out to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MVP Time\n",
    "\n",
    "A quick and dirty scraping of some data and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graphing / Plotting / Printing\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint  # pretty-printer\n",
    "import seaborn as sns\n",
    "\n",
    "# Standard Imports\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Text Analysis Packages\n",
    "import gensim\n",
    "from gensim import corpora, similarities, models\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_models(file_name='', documents=[], num_topics=5):\n",
    "    if not documents:\n",
    "        with open(file_name, 'r') as f:\n",
    "            documents = f.read().split('\\n')\n",
    "\n",
    "    # set up translator to remove punctuation\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    # remove punctuation, lowercase all words, remove stop words, stem words\n",
    "    other_stop_words = set(['theyr', 'ive', 'there', 'im', 'he', 'dont', 'id'])\n",
    "    stop_words = other_stop_words.union(ENGLISH_STOP_WORDS)\n",
    "    stemmer = PorterStemmer()\n",
    "    texts = [[stemmer.stem(word) for word in document.lower().translate(translator).split() if word not in stop_words]\n",
    "             for document in documents]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [[token for token in text if frequency[token] > 1]\n",
    "             for text in texts]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    #pprint(\"Dictionary: {}\".format(dictionary))\n",
    "    #print('\\n')\n",
    "    #pprint(\"Dictionary, Token To ID: {}\".format(dictionary.token2id))\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    # Latent Dirirchlet Allocation\n",
    "    model = models.LdaModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "    print(model)\n",
    "\n",
    "    model_tfidf = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "    print(model_tfidf)\n",
    "    \n",
    "    return model, model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Latent Semantic Indexing (LSI, or sometimes LSA)\n",
    "#lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "#corpus_lsi = lsi[corpus_tfidf]\n",
    "#model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.019*make + 0.014*vegan + 0.012*anim + 0.012*lot + 0.011*just + 0.011*peopl + 0.009*eat + 0.009*mean + 0.008*realiz + 0.008*time\n",
      "\n",
      "1: 0.026*make + 0.020*think + 0.017*your + 0.014*peopl + 0.013*vegan + 0.013*food + 0.012*just + 0.011*realli + 0.011*eat + 0.010*time\n",
      "\n",
      "2: 0.023*vegan + 0.022*just + 0.020*make + 0.014*feel + 0.014*like + 0.014*know + 0.013*thing + 0.012*peopl + 0.012*tell + 0.011*food\n",
      "\n",
      "3: 0.038*eat + 0.024*vegan + 0.018*vegetarian + 0.015*like + 0.014*want + 0.011*peopl + 0.010*anim + 0.010*restaur + 0.010*meat + 0.009*friend\n",
      "\n",
      "4: 0.039*vegan + 0.017*vegetarian + 0.016*time + 0.014*look + 0.013*year + 0.013*good + 0.011*come + 0.010*like + 0.010*bad + 0.010*your\n",
      "\n",
      "5: 0.028*meat + 0.022*eat + 0.014*tri + 0.013*vegan + 0.013*like + 0.012*peopl + 0.012*good + 0.010*famili + 0.010*tribe + 0.009*friend\n",
      "\n",
      "6: 0.033*like + 0.015*just + 0.012*cook + 0.011*make + 0.011*live + 0.010*almond + 0.009*anim + 0.008*peopl + 0.008*go + 0.008*sure\n",
      "\n",
      "7: 0.023*vegan + 0.023*eat + 0.016*just + 0.014*realli + 0.012*good + 0.011*make + 0.011*anim + 0.011*easi + 0.009*ago + 0.009*know\n",
      "\n",
      "8: 0.018*vegan + 0.017*way + 0.016*eat + 0.014*just + 0.011*meat + 0.011*start + 0.009*look + 0.009*like + 0.009*hummu + 0.008*peopl\n",
      "\n",
      "9: 0.038*vegan + 0.024*like + 0.015*peopl + 0.015*time + 0.010*chees + 0.009*best + 0.009*go + 0.009*look + 0.009*order + 0.009*make\n",
      "\n",
      "10: 0.066*vegan + 0.016*peopl + 0.014*food + 0.014*help + 0.011*know + 0.011*like + 0.011*think + 0.009*eat + 0.009*tofu + 0.009*mean\n",
      "\n",
      "11: 0.013*make + 0.013*soy + 0.011*meat + 0.011*milk + 0.010*god + 0.010*eat + 0.010*vegan + 0.010*want + 0.010*student + 0.010*work\n",
      "\n",
      "12: 0.040*vegan + 0.035*eat + 0.018*meat + 0.010*work + 0.010*food + 0.009*just + 0.009*realli + 0.009*meal + 0.009*want + 0.009*fruit\n",
      "\n",
      "13: 0.030*eat + 0.024*vegan + 0.023*peopl + 0.022*food + 0.016*use + 0.014*anim + 0.013*make + 0.012*like + 0.012*plant + 0.012*tri\n",
      "\n",
      "14: 0.037*just + 0.022*eat + 0.017*meat + 0.014*time + 0.012*think + 0.012*peopl + 0.011*know + 0.011*use + 0.010*vegan + 0.010*like\n",
      "\n",
      "15: 0.022*like + 0.016*got + 0.016*meat + 0.013*chees + 0.013*eat + 0.013*food + 0.012*just + 0.011*thing + 0.011*make + 0.010*iron\n",
      "\n",
      "16: 0.043*eat + 0.016*vegan + 0.014*want + 0.014*meat + 0.013*diet + 0.012*food + 0.010*peopl + 0.010*vegetarian + 0.009*think + 0.009*say\n",
      "\n",
      "17: 0.020*just + 0.019*eat + 0.017*vegan + 0.016*meat + 0.014*make + 0.014*anim + 0.012*thing + 0.012*happen + 0.012*like + 0.010*peopl\n",
      "\n",
      "18: 0.023*make + 0.023*need + 0.016*meat + 0.015*know + 0.012*gluten + 0.012*use + 0.011*peopl + 0.010*just + 0.010*mayb + 0.009*dine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(model.num_topics - 1):\n",
    "    print(\"{0}: {1}\\n\".format(i, model.print_topic(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.009*do + 0.009*know + 0.008*egg + 0.008*base + 0.007*like + 0.007*2 + 0.007*cup + 0.007*luck + 0.006*recip + 0.006*vegan\n",
      "\n",
      "1: 0.013*tofu + 0.011*hummu + 0.010*vegan + 0.010*product + 0.009*problem + 0.008*peopl + 0.008*funni + 0.008*error + 0.008*chees + 0.008*lentil\n",
      "\n",
      "2: 0.018*eat + 0.011*food + 0.008*point + 0.008*thing + 0.007*meat + 0.007*hummu + 0.007*ill + 0.007*vegan + 0.006*buy + 0.006*mean\n",
      "\n",
      "3: 0.010*ugh + 0.009*restaur + 0.009*make + 0.009*your + 0.009*got + 0.007*know + 0.007*time + 0.006*annoy + 0.006*that + 0.006*anim\n",
      "\n",
      "4: 0.013*vegan + 0.010*food + 0.008*make + 0.008*hummu + 0.008*eat + 0.007*feel + 0.006*like + 0.006*just + 0.006*meat + 0.006*think\n",
      "\n",
      "5: 0.019*vegan + 0.009*ye + 0.007*ask + 0.007*tofu + 0.007*chees + 0.006*egg + 0.006*live + 0.006*milk + 0.006*mean + 0.006*famili\n",
      "\n",
      "6: 0.011*look + 0.009*recip + 0.007*suck + 0.007*make + 0.007*pretti + 0.007*vegan + 0.007*eat + 0.007*didnt + 0.007*nurs + 0.007*go\n",
      "\n",
      "7: 0.009*time + 0.009*complaint + 0.007*order + 0.006*brand + 0.006*data + 0.006*view + 0.006*eat + 0.006*problem + 0.005*smoothi + 0.005*chicken\n",
      "\n",
      "8: 0.012*veget + 0.009*meat + 0.008*fruit + 0.007*know + 0.007*5 + 0.006*basic + 0.006*chang + 0.006*want + 0.006*plantbas + 0.006*vegan\n",
      "\n",
      "9: 0.010*person + 0.009*vegan + 0.008*vegetarian + 0.007*know + 0.007*wouldnt + 0.007*fish + 0.007*omni + 0.007*make + 0.006*way + 0.006*omg\n",
      "\n",
      "10: 0.016*just + 0.011*use + 0.009*eat + 0.009*vegan + 0.008*peopl + 0.006*make + 0.006*have + 0.006*kitchen + 0.006*feel + 0.006*want\n",
      "\n",
      "11: 0.013*realli + 0.010*vegetarian + 0.009*delet + 0.009*live + 0.007*cruelti + 0.007*warm + 0.007*lot + 0.007*bitch + 0.006*help + 0.006*dish\n",
      "\n",
      "12: 0.020*vegan + 0.011*friend + 0.010*food + 0.007*teriyaki + 0.007*just + 0.007*eat + 0.006*peopl + 0.006*burger + 0.006*live + 0.005*tell\n",
      "\n",
      "13: 0.011*make + 0.010*easi + 0.008*mushroom + 0.008*tri + 0.008*vegan + 0.007*vegetarian + 0.007*mistak + 0.006*human + 0.006*hungri + 0.006*fail\n",
      "\n",
      "14: 0.017*vegan + 0.008*toast + 0.008*best + 0.007*polit + 0.007*peopl + 0.007*work + 0.006*post + 0.006*usual + 0.006*cute + 0.006*eat\n",
      "\n",
      "15: 0.009*spice + 0.009*sorri + 0.008*cring + 0.008*eggplant + 0.007*save + 0.007*banana + 0.006*vegan + 0.006*use + 0.006*good + 0.006*happen\n",
      "\n",
      "16: 0.009*meat + 0.007*thing + 0.007*eat + 0.007*gluten + 0.007*vegetarian + 0.007*like + 0.006*vegan + 0.006*make + 0.006*ask + 0.006*fake\n",
      "\n",
      "17: 0.014*meat + 0.010*toast + 0.010*soy + 0.009*just + 0.009*feel + 0.008*like + 0.008*eat + 0.008*milk + 0.007*need + 0.006*recip\n",
      "\n",
      "18: 0.026*delet + 0.013*crab + 0.012*like + 0.007*good + 0.007*think + 0.006*just + 0.006*vegan + 0.006*make + 0.006*realli + 0.005*want\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(model_tfidf.num_topics - 1):\n",
    "    print(\"{0}: {1}\\n\".format(i, model_tfidf.print_topic(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=517, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=517, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-animalrights-subreddit.txt MODEL\n",
      "Topic 0: vegan, anim, peopl, meat, think, pig, help, like, eat, ban\n",
      "Topic 1: anim, like, abus, cat, report, sub, hope, peopl, good, feel\n",
      "Topic 2: anim, right, human, feel, cat, like, activist, test, just, help\n",
      "Topic 3: anim, peopl, right, live, make, kill, help, new, want, vegan\n",
      "\n",
      "\n",
      "SUBRED data/mvp-animalrights-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: anim, abus, kill, hunt, think, use, read, peopl, meat, wild\n",
      "Topic 1: anim, right, welfar, report, realli, hope, world, group, good, extinct\n",
      "Topic 2: human, anim, peopl, judg, right, milk, protect, fight, action, vegan\n",
      "Topic 3: vegan, anim, feel, right, ranimalwelfar, like, check, cat, big, free\n",
      "\n",
      "\n",
      "***************************\n",
      "LdaModel(num_terms=520, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=520, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-animalwelfare-subreddit.txt MODEL\n",
      "Topic 0: anim, cat, care, pet, shelter, need, help, veterinari, vet, just\n",
      "Topic 1: help, good, dog, cat, anim, bunni, think, credit, card, vet\n",
      "Topic 2: anim, cat, dog, just, like, want, work, believ, wild, look\n",
      "Topic 3: dog, anim, help, like, just, need, shelter, cat, realli, donat\n",
      "\n",
      "\n",
      "SUBRED data/mvp-animalwelfare-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: cat, dog, help, need, kill, select, care, pet, natur, say\n",
      "Topic 1: dog, inform, help, interest, veterinari, puppi, tri, go, hope, mayb\n",
      "Topic 2: good, anim, read, delet, cat, dog, just, stop, kill, friend\n",
      "Topic 3: anim, shelter, pet, vet, cat, tell, just, cruelti, expect, rabbit\n",
      "\n",
      "\n",
      "***************************\n",
      "LdaModel(num_terms=380, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=380, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-veg-subreddit.txt MODEL\n",
      "Topic 0: anim, vegan, meat, peopl, kid, better, diet, eat, probabl, thing\n",
      "Topic 1: vegan, food, like, peopl, make, diet, meat, chicken, tri, vegetarian\n",
      "Topic 2: anim, meat, eat, just, risk, vitamin, 1, 3, veget, 2\n",
      "Topic 3: vegan, vegetarian, anim, peopl, chicken, make, 1, industri, meat, product\n",
      "\n",
      "\n",
      "SUBRED data/mvp-veg-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: anim, meat, stop, know, just, say, pumpkin, look, vegan, cultur\n",
      "Topic 1: vegan, vegetarian, make, green, meat, diet, food, love, homemad, 1\n",
      "Topic 2: make, food, vegan, link, peopl, thing, eat, anim, summer, better\n",
      "Topic 3: 1, bunch, fridg, tri, appl, vegan, rais, bag, peopl, nutrit\n",
      "\n",
      "\n",
      "***************************\n",
      "LdaModel(num_terms=1326, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=1326, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-vegetarian-subreddit.txt MODEL\n",
      "Topic 0: eat, meat, vegetarian, look, like, egg, meal, just, vegan, fish\n",
      "Topic 1: eat, vegetarian, make, meat, your, diet, just, thing, use, cook\n",
      "Topic 2: good, like, vegetarian, eat, vegan, anim, realli, food, think, meat\n",
      "Topic 3: vegetarian, meat, make, peopl, eat, time, vegan, like, tri, know\n",
      "\n",
      "\n",
      "SUBRED data/mvp-vegetarian-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: look, make, meat, great, good, realli, fish, recip, food, egg\n",
      "Topic 1: vegetarian, eat, meat, like, good, vegan, cook, food, recip, make\n",
      "Topic 2: vegetarian, lettuc, vegan, anim, look, make, diet, eat, like, meat\n",
      "Topic 3: vegetarian, meat, eat, help, vegan, good, make, recip, tri, meal\n",
      "\n",
      "\n",
      "***************************\n",
      "LdaModel(num_terms=579, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=579, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-vegetarianism-subreddit.txt MODEL\n",
      "Topic 0: milk, vegetarian, almond, vegan, meat, tri, recip, good, eat, like\n",
      "Topic 1: recip, anim, make, sandwich, love, eat, thank, food, corn, veg\n",
      "Topic 2: meat, eat, vegan, food, recip, year, vegetarian, anim, tri, just\n",
      "Topic 3: eat, just, peopl, food, tri, make, vegetarian, realli, chicken, meat\n",
      "\n",
      "\n",
      "SUBRED data/mvp-vegetarianism-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: eat, recip, vegan, meat, thank, vegetarian, good, 2016, anim, peopl\n",
      "Topic 1: amaz, vegan, recip, 12, make, tortilla, just, add, pumpkin, tri\n",
      "Topic 2: milk, easi, recip, hey, delici, just, know, red, 1, idea\n",
      "Topic 3: vegan, meat, food, great, eat, soy, thought, new, better, festiv\n",
      "\n",
      "\n",
      "***************************\n",
      "LdaModel(num_terms=1, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=1, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-dietaryvegan-subreddit.txt MODEL\n",
      "Topic 0: recip\n",
      "Topic 1: recip\n",
      "Topic 2: recip\n",
      "Topic 3: recip\n",
      "\n",
      "\n",
      "SUBRED data/mvp-dietaryvegan-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: recip\n",
      "Topic 1: recip\n",
      "Topic 2: recip\n",
      "Topic 3: recip\n",
      "\n",
      "\n",
      "***************************\n",
      "LdaModel(num_terms=413, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=413, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-veganrecipes-subreddit.txt MODEL\n",
      "Topic 0: recip, make, add, potato, sweet, good, banana, 2, tomato, thank\n",
      "Topic 1: add, recip, cook, water, minut, oil, vegan, larg, use, delici\n",
      "Topic 2: 1, mushroom, recip, look, minut, dough, add, vegan, fill, bit\n",
      "Topic 3: vegan, cup, chocol, recip, pumpkin, add, cook, wine, 2, chees\n",
      "\n",
      "\n",
      "SUBRED data/mvp-veganrecipes-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: vegan, recip, cook, powder, browni, serv, make, look, great, cashew\n",
      "Topic 1: look, ingredi, appl, thank, dip, 6, cream, raspberri, high, vegan\n",
      "Topic 2: walnut, vegan, banana, recip, cinnamon, add, ingredi, halloween, includ, pancak\n",
      "Topic 3: 2, delici, chocol, cup, bread, banana, minc, instruct, pumpkin, vegan\n",
      "\n",
      "\n",
      "***************************\n",
      "LdaModel(num_terms=1308, num_topics=5, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=1308, num_topics=5, decay=0.5, chunksize=2000)\n",
      "SUBRED data/mvp-vegproblems-subreddit.txt MODEL\n",
      "Topic 0: vegan, like, just, eat, peopl, time, vegetarian, feel, anim, use\n",
      "Topic 1: make, just, eat, meat, anim, feel, realli, like, know, peopl\n",
      "Topic 2: vegan, eat, peopl, food, make, meat, use, think, vegetarian, just\n",
      "Topic 3: eat, vegan, meat, make, food, like, tofu, good, peopl, know\n",
      "\n",
      "\n",
      "SUBRED data/mvp-vegproblems-subreddit.txt MODEL_TFIDF\n",
      "Topic 0: delet, eat, vegan, make, meat, live, use, thing, realli, anim\n",
      "Topic 1: vegan, think, just, eat, peopl, like, thing, good, meat, tofu\n",
      "Topic 2: vegan, chicken, eat, need, make, meat, know, tofu, feel, peopl\n",
      "Topic 3: vegan, food, make, eat, like, meat, peopl, vegetarian, soy, just\n",
      "\n",
      "\n",
      "***************************\n"
     ]
    }
   ],
   "source": [
    "file_name_template = os.path.join('data', 'mvp-{}-subreddit.txt')\n",
    "subreds = [\n",
    "        'animalrights',\n",
    "        'animalwelfare',\n",
    "        'veg',\n",
    "        'vegetarian',\n",
    "        'vegetarianism',\n",
    "        'dietaryvegan',\n",
    "        'veganrecipes',\n",
    "        'vegproblems',\n",
    "    ]\n",
    "file_names = [file_name_template.format(subred) for subred in subreds]\n",
    "\n",
    "for file_name in file_names:\n",
    "    model, model_tfidf = get_models(file_name)\n",
    "    print('SUBRED {} MODEL'.format(file_name))\n",
    "    for i in range(model.num_topics - 1):\n",
    "        print('Topic {0}: {1}'.format(i, ', '.join([x[0] for x in model.show_topic(i)])))\n",
    "    print('\\n')\n",
    "    print('SUBRED {} MODEL_TFIDF'.format(file_name))\n",
    "    for i in range(model.num_topics - 1):\n",
    "        print('Topic {0}: {1}'.format(i, ', '.join([x[0] for x in model_tfidf.show_topic(i)])))\n",
    "    print('\\n')\n",
    "    print('***************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing topics between different subreddits\n",
    "\n",
    "Using the whole subreddit corpus as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=2952, num_topics=10, decay=0.5, chunksize=2000)\n",
      "LdaModel(num_terms=2952, num_topics=10, decay=0.5, chunksize=2000)\n",
      "Topic 0: eat, vegetarian, anim, vegan, just, like, meat, make, good, food\n",
      "Topic 1: like, anim, vegan, eat, vegetarian, make, just, meat, peopl, help\n",
      "Topic 2: meat, vegan, eat, like, vegetarian, just, make, peopl, good, anim\n",
      "Topic 3: vegan, eat, meat, anim, like, just, food, make, vegetarian, peopl\n",
      "Topic 4: meat, eat, vegan, vegetarian, like, make, anim, food, just, realli\n",
      "Topic 5: vegan, eat, anim, vegetarian, just, make, like, meat, recip, tri\n",
      "Topic 6: vegan, anim, eat, make, like, meat, vegetarian, just, peopl, food\n",
      "Topic 7: vegan, eat, meat, vegetarian, like, just, anim, make, peopl, food\n",
      "Topic 8: anim, just, meat, like, peopl, make, vegetarian, eat, vegan, food\n",
      "\n",
      "\n",
      "Topic 0: meat, vegan, eat, just, anim, butter, hummu, diet, like, dad\n",
      "Topic 1: shelter, cat, vet, veterinari, anim, dog, bunni, uc, pet, stray\n",
      "Topic 2: vegan, anim, wool, diet, meat, sweater, tyson, carnivor, child, male\n",
      "Topic 3: pineappl, rvegetarian, meateat, girlfriend, rice, recip, x, favorit, breakfast, vegan\n",
      "Topic 4: meat, anim, diet, vegetarian, vegan, eat, egg, dish, like, make\n",
      "Topic 5: meateat, pineappl, recip, rvegetarian, girlfriend, x, rice, favorit, sesam, anim\n",
      "Topic 6: pineappl, rvegetarian, x, meateat, girlfriend, recip, anim, rice, favorit, breakfast\n",
      "Topic 7: anim, cat, protest, report, meat, vegan, shelter, lion, eat, peopl\n",
      "Topic 8: recip, pineappl, meateat, rvegetarian, girlfriend, x, favorit, rice, breakfast, sesam\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for file_name in file_names:\n",
    "    with open(file_name, 'r') as f:\n",
    "        documents.append(f.read().replace('\\n', ' '))\n",
    "\n",
    "model, model_tfidf = get_models(documents=documents, num_topics=10)\n",
    "\n",
    "for i in range(model.num_topics - 1):\n",
    "    print('Topic {0}: {1}'.format(i, ', '.join([x[0] for x in model.show_topic(i)])))\n",
    "print('\\n')\n",
    "for i in range(model.num_topics - 1):\n",
    "    print('Topic {0}: {1}'.format(i, ', '.join([x[0] for x in model_tfidf.show_topic(i)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pressbot-env]",
   "language": "python",
   "name": "conda-env-pressbot-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
